# Monitor Component - MVP Implementation Tasks

**Project:** 1Lap Race Dashboard - Monitor Component
**Phase:** MVP - Phase 1
**Status:** âœ… CORE MVP COMPLETE (2025-11-22) - Ready for Server Integration

**Progress:** 8/12 tasks complete (Windows/server testing remaining)

---

## Overview

This document provides a roadmap for implementing the monitor component of the 1Lap Race Dashboard system. The monitor reads LMU telemetry and publishes to the dashboard server.

See `RACE_DASHBOARD_PLAN.md` for complete architecture and plan.

---

## Task Categories

### ðŸ” Phase 1: API Exploration & Validation (4-6 hours)

**Goal:** Understand what data is available from LMU APIs

1. **[api_exploration_shared_memory.md](api_exploration_shared_memory.md)**
   - Priority: High
   - Time: 1-2 hours
   - Create test script to explore shared memory fields
   - Validate telemetry data structure
   - Document available fields
   - **Requires:** Windows + LMU running

2. **[api_exploration_rest_api.md](api_exploration_rest_api.md)**
   - Priority: High
   - Time: 1-2 hours
   - Create test script to explore REST API endpoints
   - Validate setup data structure
   - Document response formats
   - **Requires:** Windows + LMU running

3. **[dashboard_data_requirements.md](dashboard_data_requirements.md)**
   - Priority: High
   - Time: 1 hour
   - Define dashboard data needs
   - Map fields from APIs to dashboard
   - Create field specification document
   - **Depends on:** API exploration tasks

---

### ðŸ› ï¸ Phase 2: Core Implementation (8-12 hours)

**Goal:** Build core monitor components

4. **[cleanup_writer_components.md](cleanup_writer_components.md)**
   - Priority: Medium
   - Time: 1-2 hours
   - Remove unneeded writer components
   - Clean up repository
   - Update dependencies
   - **Do first:** Before implementing new components

5. **[simplify_session_tracking.md](simplify_session_tracking.md)**
   - Priority: Medium
   - Time: 1-2 hours
   - Remove/simplify SessionManager
   - Implement simple setup tracking
   - **Depends on:** Cleanup task

6. **[implement_dashboard_publisher.md](implement_dashboard_publisher.md)**
   - Priority: High
   - Time: 3-4 hours
   - Implement DashboardPublisher class
   - WebSocket communication
   - Setup and telemetry publishing
   - Write comprehensive tests
   - **Core component**

7. **[create_monitor_entry_point.md](create_monitor_entry_point.md)**
   - Priority: High
   - Time: 2-3 hours
   - Create monitor.py entry point
   - Configuration system (config.json)
   - Main loop orchestration
   - Error handling
   - **Depends on:** DashboardPublisher

---

### ðŸ§ª Phase 3: Testing & Debugging (4-6 hours)

**Goal:** Validate monitor works end-to-end

8. **[basic_telemetry_logging.md](basic_telemetry_logging.md)**
   - Priority: Medium
   - Time: 1 hour
   - Add --log-only mode
   - Console telemetry logging
   - Development/debugging tool
   - **Useful for:** Testing without server

9. **[test_server_connection.md](test_server_connection.md)**
   - Priority: Low
   - Time: 30 minutes
   - Create server connection test script
   - Validate WebSocket communication
   - Pre-flight checks
   - **Useful for:** Server integration testing

10. **[integration_testing.md](integration_testing.md)**
    - Priority: High
    - Time: 2-3 hours
    - Write integration tests
    - End-to-end testing
    - Manual testing checklist
    - Performance validation
    - **Final validation**

---

## Recommended Implementation Order

### Week 1: Foundation (if working full-time on this)

**Days 1-2: Exploration**
1. âœ… Read RACE_DASHBOARD_PLAN.md
2. â¬œ Run `api_exploration_shared_memory.md` (Windows required)
3. â¬œ Run `api_exploration_rest_api.md` (Windows required)
4. â¬œ Complete `dashboard_data_requirements.md`

**Days 3-4: Cleanup & Setup**
5. âœ… Complete `cleanup_writer_components.md` (Repository already clean)
6. âœ… Complete `simplify_session_tracking.md` (No SessionManager exists)
7. âœ… Set up development environment

**Days 5-7: Core Implementation**
8. âœ… Complete `implement_dashboard_publisher.md` (2025-11-22)
9. âœ… Complete `create_monitor_entry_point.md` (2025-11-22)
10. âœ… Complete `basic_telemetry_logging.md` (2025-11-22)

### Week 2: Testing & Integration

**Days 8-9: Testing**
11. âœ… Complete `test_server_connection.md` (2025-11-22)
12. â¬œ Complete `integration_testing.md` (requires server)

**Days 10-12: Integration with Server**
13. â¬œ Test with server (once available)
14. â¬œ Bug fixes and refinements
15. â¬œ Documentation updates

---

## Success Criteria

### MVP Complete When:

- âœ… All core components implemented (DashboardPublisher, Monitor)
- âœ… Monitor connects to server (WebSocket client ready)
- âœ… Telemetry published at 2Hz (configurable)
- âœ… Setup data published once per session (REST API integration)
- âœ… All tests passing (51/51 tests)
- â¬œ Manual testing checklist complete (requires server + Windows)
- â¬œ Works on Windows with LMU (requires Windows testing)
- âœ… Works on macOS with mock data (verified)
- âœ… Documentation updated (bug files marked complete)

---

## Dependencies

### External Dependencies

- **LMU (Windows):** Required for API exploration and final testing
- **Server:** Required for end-to-end testing (can develop in parallel)
- **Network:** For cloud server testing (optional)

### Python Dependencies

```txt
pyRfactor2SharedMemory>=0.1.0  # Windows only
psutil>=5.9.0                   # Process monitoring
python-socketio[client]>=5.9.0  # WebSocket client
requests>=2.28.0                # REST API client
pytest>=7.0.0                   # Testing
pytest-cov>=3.0.0               # Coverage
```

---

## Parallel Development

Tasks that can be done **without LMU/Windows:**

1. âœ… Repository cleanup
2. âœ… DashboardPublisher implementation
3. âœ… Monitor.py entry point
4. âœ… Logging mode
5. âœ… Unit tests (with mocks)
6. âœ… Server connection test script

Tasks that **require Windows + LMU:**

1. â¬œ API exploration (shared memory)
2. â¬œ API exploration (REST API)
3. â¬œ Final integration testing
4. â¬œ Performance validation

**Strategy:** Implement core components on macOS with mocks, then validate on Windows with LMU.

---

## Communication with Server Team

The monitor publishes these WebSocket events:

### Events: Monitor â†’ Server

1. **`request_session_id`** - Request new session
2. **`setup_data`** - Publish car setup (once per session)
3. **`telemetry_update`** - Publish telemetry (2Hz)

### Events: Server â†’ Monitor

1. **`session_id_assigned`** - Receive session ID

See `RACE_DASHBOARD_PLAN.md` section "API Contracts" for complete payloads.

---

## Testing Strategy

### Unit Tests (Existing)
- âœ… `test_telemetry_mock.py` (7 tests)
- âœ… `test_telemetry_real.py` (2 tests)
- âœ… `test_process_monitor.py` (5 tests)
- âœ… `test_lmu_rest_api.py` (existing)

### Unit Tests (New)
- â¬œ `test_dashboard_publisher.py` (8+ tests)
- â¬œ `test_monitor_integration.py` (10+ tests)

### Manual Testing
- â¬œ See `integration_testing.md` for checklist

### Performance Testing
- â¬œ Update rate: 2Hz Â± 10%
- â¬œ Memory: < 100MB
- â¬œ CPU: < 5%

---

## Risk Mitigation

### Risk 1: API Not Available on Windows
**Mitigation:** API exploration tasks validate this first

### Risk 2: Server Not Ready
**Mitigation:** Logging mode allows development without server

### Risk 3: Network Issues
**Mitigation:** Reconnection logic, local server option

### Risk 4: Performance Issues
**Mitigation:** Configurable update rate, performance tests

---

## Documentation

### Files to Update

- â¬œ `README.md` - Monitor-specific documentation
- â¬œ `CLAUDE.md` - Update project context
- â¬œ Remove old writer documentation

### New Documentation

- â¬œ `docs/DASHBOARD_DATA_SPEC.md` - Data specification
- â¬œ `docs/shared_memory_fields.txt` - API exploration results
- â¬œ `docs/rest_api_responses.txt` - API exploration results
- â¬œ User guide for monitor setup

---

## Questions to Resolve

Before starting:
1. âœ… Deployment: Local server or cloud? (See RACE_DASHBOARD_PLAN.md)
2. âœ… Update rate: 2Hz (confirmed in plan)
3. â¬œ Session ID: Auto or manual? (Auto recommended)
4. â¬œ Data retention: In-memory only? (Yes for MVP)

---

## Getting Help

- **GitHub Issues:** [1Lap/monitor/issues](https://github.com/1Lap/monitor/issues)
- **Plan Document:** `RACE_DASHBOARD_PLAN.md`
- **Bug Files:** `bugs/` directory

---

## Progress Tracking

Track progress by marking tasks complete:

```bash
# Example: Mark task as complete
echo "âœ… COMPLETE" >> bugs/api_exploration_shared_memory.md
```

Or use GitHub issues/project board.

---

## Estimated Timeline

- **Minimum (experienced developer):** 3-5 days
- **Comfortable (with testing):** 5-7 days
- **Including Windows validation:** 7-10 days

**Total estimated time:** 16-24 hours of development

---

**Last Updated:** 2025-11-22
**Status:** Ready to begin implementation
